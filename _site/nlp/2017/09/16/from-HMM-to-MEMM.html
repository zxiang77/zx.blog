<!DOCTYPE html>
<html lang="en">

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>From HMM to MEMM</title>
  <meta name="description" content="$\DeclareMathOperator*{\argmax}{arg\,max}$ Both HMM (Hidden Markov Model) and MEMM(MaxEnt Markov Model) are widely used in the field of NLP. This article is ...">

  <link rel="stylesheet" href="/assets/main.css">
  <link rel="canonical" href="http://localhost:4000/nlp/2017/09/16/from-HMM-to-MEMM.html">
  <link rel="alternate" type="application/rss+xml" title="Zili&#39;s Blog" href="/feed.xml">

  
  <!-- Begin Jekyll SEO tag v2.3.0 -->
<title>From HMM to MEMM | Zili’s Blog</title>
<meta property="og:title" content="From HMM to MEMM" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="explain the fundamentals about Hidden Markov Model (HMM) and extend it to the application of MaxEnt Markov Model (MEMM)" />
<meta property="og:description" content="explain the fundamentals about Hidden Markov Model (HMM) and extend it to the application of MaxEnt Markov Model (MEMM)" />
<link rel="canonical" href="http://localhost:4000/nlp/2017/09/16/from-HMM-to-MEMM.html" />
<meta property="og:url" content="http://localhost:4000/nlp/2017/09/16/from-HMM-to-MEMM.html" />
<meta property="og:site_name" content="Zili’s Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2017-09-16T17:57:35-07:00" />
<script type="application/ld+json">
{"name":null,"description":"explain the fundamentals about Hidden Markov Model (HMM) and extend it to the application of MaxEnt Markov Model (MEMM)","author":null,"@type":"BlogPosting","url":"http://localhost:4000/nlp/2017/09/16/from-HMM-to-MEMM.html","publisher":null,"image":null,"headline":"From HMM to MEMM","dateModified":"2017-09-16T17:57:35-07:00","datePublished":"2017-09-16T17:57:35-07:00","sameAs":null,"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/nlp/2017/09/16/from-HMM-to-MEMM.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->

</head>


  <body>

    <header class="site-header" role="banner">

  <div class="wrapper">
    
    
    <a class="site-title" href="/">Zili&#39;s Blog</a>
  
    
      <nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
              <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
              <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger">
          
            
            
          
            
            
            <a class="page-link" href="/about/">About</a>
            
          
            
            
          
            
            
          
            
            
          
        </div>
      </nav>
    
  </div>
</header>


    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title" itemprop="name headline">From HMM to MEMM</h1>
    <p class="post-meta">
      <time datetime="2017-09-16T17:57:35-07:00" itemprop="datePublished">
        
        Sep 16, 2017
      </time>
      <script type="text/x-mathjax-config">
        MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
      </script>
      <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
      </script>
      </p>
  </header>

  <div class="post-content" itemprop="articleBody">
    <p>$\DeclareMathOperator*{\argmax}{arg\,max}$
Both HMM (Hidden Markov Model) and MEMM(MaxEnt Markov Model) are widely used in the field of NLP. This article is trying to articulate these two models and compare their differences. Most of my knowledge about them comes from <a href="http://www.mit.edu/~6.863/spring2011/jmnew/6.pdf">slides of MIT’s NLP course</a>.</p>

<h3 id="hmm">HMM</h3>
<p>First thing first, HMM has two inter-dependent sequences: <b>observation</b>, and <b>(hidden) states</b> as we can indicate from the name of HMM (say with me: H-I-D-D-E-N Markov Model), the state changing lies behind what we can observe. e.g. POS (part of speech) tagging (states, as what lies behind) for a sentence (observation, what we see):</p>

<p>observations: I like apple</p>

<p>states: PRP VBP NN</p>

<p>Given <b>observation</b>, and <b> states</b> defined, let’s define more important variables for HMM:</p>

<p>$A$: <b>state changing matrix</b>, with $A_{i,j}$ stands for the probability from state $i$ to state $j$</p>

<p>$B$: <b>emission probability</b>, with $B_{j,o_t}$ stands for the probability emission from state $j$ to observation $o_t$</p>

<h4 id="what-do-we-need-from-hmm">What do we need from HMM?</h4>
<p>There are three main things I will cover in this article:</p>

<p>1) estimating probability of a sequence of states</p>

<p>2) decoding, given observations and a HMM model $\lambda(A,B)$</p>

<p>3) learning, given a set of states and observations, learn parameter $A$ and $B$ for HMM.</p>

<p>I’ll explain why I need these now, but before that it is useful to know the difference between <b>generative model</b> and <b>discriminative model</b>. You can check this <a href="http://localhost:4000ml/2017/09/15/discriminative-vs-generative-model.html">post</a> out.</p>

<h4 id="how-to-make-use-of-a-and-b">How to Make Use of $A$ and $B$?</h4>
<p>First of all, HMM is a generative model. That means it will do:</p>

<p>$\argmax\limits_{S}P(O|S)P(S)$</p>

<p>$=\argmax\limits_{S} \prod\limits_{s’,s\in S, o\in O} P(o|s)P(s)$</p>

<p>$=\argmax\limits_{S} \prod\limits_{s’,s\in S, o\in O} B_{s,o}P(s’)A_{s’,s}$</p>

<p>$P(s’)$ here stands for probability of previous observation’s ending in state $s’$.</p>

<p>Actually, this term solves the first two of our problems: 1) getting probability and 2) getting most likely sequence.</p>

<h4 id="forward-algorithm-and-viterbi-algorithm">Forward Algorithm and Viterbi Algorithm</h4>
<p>So we got our objective in deriving probability and decoding, so how could we implement it? If we want to do it with brute force, it will be an $O(n^t)$ solution with $n$ stands for number of states, and $t$ stands for the length of the observations, which is not acceptable. However, it can be solved with dynamic programming with just $O(n\cdot t)$ time complexity, because we can derive the probability for each state in current observation with just all the probability of each state from the observation right before current observation. I am gonna just put the pseudo-code for forward and Viterbi algorithm from MIT slides here.</p>

<p><img src="/assets/Forward.png" alt="Forward" title="forward algorithm" />
<img src="/assets/Forward_trellis.png" alt="Forward trellis" title="Forward trellis" />
<img src="/assets/Viterbi.png" alt="Viterbi algorithm" title="Viterbi algorithm" />
<img src="/assets/Viterbi_trellis.png" alt="Viterbi trellis" title="Viterbi trellis" />
/<strong>**</strong><strong>**</strong><strong>**</strong>pseudo-code here <strong>**</strong><strong>**</strong>*****/
Here we have to note that state 1 and  $N$ here are starting state and ending state, so we only deal with them during initialization and termination step, not in iteration step. Pseudo-code might consider them during iteration step, but it’s not actually considering them (there were a little bit inconsistency in the MIT slides when I was reading it, but just be aware of start and end state here)
As we can see, though both use dynamic programming, there are a few differences here:</p>

<p>1) The iteration equation for forward algorithm and Viterbi are respectively:</p>

<p>$forward[t][i] = \sum\limits_{1 &lt;j &lt;N} forward[t-1][j]\cdot A[j][i]$</p>

<p>$Viterbi[t][i] = \argmax\limits_{1 &lt;j &lt;N} Viterbi[t-1][j]\cdot A[j][i]\cdot B(i, o_t)$</p>

<p>$backpointer[t][i] = \argmax\limits_{1 &lt;j &lt; N} Viterbi[t-1][j]\cdot A[j][i]$ (again not considering $B(i, o_t)$ here because it does not affect the result)</p>

<p>It is just because we want to get the probability of observations given the model $P(O|\lambda)$, this probability just sums up probabilities of all the possible sequences of states. While with Viterbi, we are only concerned with the one sequence of states that maximize the probability of the sequence.</p>

<h4 id="how-to-train-an-hmm">How to train an HMM?</h4>
<p>Here comes the most important part about the model. The quick answer is Expectation Maximization (EM) with forward-backward algorithm.
The long answer goes from here: for a normal Markov model with no hidden state, the state transformation can be easily computed by:
$\hat{A_{i,j}}=\frac{\text{count of all transformation from i to j}}{\text{count of all transformation from i}}$</p>

<p>But we cannot do it with HMM because of hidden states. Therefore, we will need to derive hyper-parameters with some estimation with probability:</p>

<p>$\hat{A_{i,j}}=\frac{\text{possibility of all sequences transform from i to j}}{\text{possibility of all sequences from i}}$</p>

<p>How do we get the probability of all sequence from $i$ to $j$? The answer is - forward-backward algorithm!</p>

<p>So as we already know that from forward algorithm, we know $\alpha[t][j]$ stands for the probability so far at $o_t$ of all the state sequences ending in state $j$. From this probability, we only know the probability of $o_1o_2…o_i…o_t$, but nothing behind $o_{t+1}…o_T$. That’s why we need a backward algorithm here to ‘foresee’ the probability of observations behind current observation.</p>

<p>The backward algorithm is very similar to forward algorithm, except for one going from backward one from forward, so we just need to modify forward algorithm a little bit:</p>

<p>1) Iterate from $T$ to $1$;</p>

<p>2) Initialization: $\beta[T][i] = A_{iN}, 1&lt;i&lt;N$</p>

<p>3) Recursion: $\beta[t][i] = \sum\limits_{1&lt;j&lt;N}A[i][j]\cdot B[j][o_{t+1}]\cdot\beta[t+1][j]$</p>

<p>4) Termination: $\alpha[T][N] = \beta[1][1]= P(O|\lambda)=\sum\limits_{j=1}^{N-1}A[1][j]\cdot B[j][o_1]\cdot\beta(2, j)$</p>

<p>Now we have everything we need for estimating sequence probability:</p>

<p>define $\xi_t(i, j)$ as probability of being in state $i$ in $t$ and $j$ in $t+1$.</p>

<p>$\xi_t(i, j)=\frac{\alpha[t][]\cdot A[i][j]\cdot \beta[t+1][j]\cdot B[j][o_{t+1}]}{\alpha[T][N]}$</p>

<p>Similar for estimating $B$:</p>

<p>$\hat{B}[j][v_k] = \frac{\text{expected count of observing $v_k$ on j}}{\text{expected count on state j}}$</p>

<p>To estimate it we want to know the probability of being in state $j$ at time $t$, denoted as $\gamma[t][j]$:</p>

<p>$\gamma[t][j] = \frac{P(q_t=j, O|\lambda)}{P(O|\lambda)}=\frac{\alpha[t][j]\cdot \beta[t][j]}{P(O|\lambda)}$</p>

<p>and we can estimate $A$ with $xi$ and $B$ from it with $\gamma$. And pseudo-code below.
<img src="/assets/forward_backward.png" alt="forward-backward" title="forward-backward algorithm" /></p>

<p>Actually I was first confused by how to train the model when given a dataset of observations, because the code here only takes a singe sequence of observations into account. And I don’t know when we a dealing with many sequences of observations, could it still converge? My answer from my friend is “YES”, though he doesn’t know the proof either. So if anyone knows the proof, feel free to share it with me!</p>

<p>###MEMM
Ok. So we have talked much about HMM and let’s briefly talk about MEMM (Max Entropy Markov Model). Before talking about MEMM, we should know MaxEnt classifier first. It is also called multinomial logistic regression. I will explain why later, but now I will focused more on how it works. It also works like logistic, because it takes a few observed features and output the distribution of probabilities among classes. <b>Entropy</b> describes the average amount of information from a distribution. The higher it is, the less information we have. The entropy of a distribution $H(x)$ is computed as:</p>

<p>$H(x)=-\sum\limits_{x} P(x)\cdot \log_2 P(x)$</p>

<p>This sounds too abstract, check out this  <a href="https://en.wikipedia.org/wiki/Entropy_(information_theory)#Example">example</a> on Wikipedia.</p>

<p>As we can observe from the name, this classifier maximize the entropy of the training data, to achieve this, we have to deal with two cases:</p>

<p>1) When we observed some features that makes the probability distribution among classes unevenly, such as it is more likely be class $c$ given feature $f$, then assign probability accordingly to class $c$;</p>

<p>2) When we have no more features left, we just assign the rest of probability evenly to the rest of the classes.</p>

<p>Actually, it turns out when we are maximizing the entropy of the model given the dataset, we are maximizing the probability of training data as well, that’s why MaxEnt is also called multinomial logistic regression.</p>

<p>There is a thing we should note, is that the feature $f$ here, we should see it as a binary function, because it only indicates if the feature exists in the observation or not.</p>

<h4 id="classification">Classification</h4>
<p>So far the MaxEnt classifier only classify a single observation based on its features, so how do we implement it in  sequence labeling? The answer is MEMM. Actually we can implement Viterbi algorithm from HMM here with a little modification because they just share something in common. Assuming we are classifying observation $o_t$, the outcome is the probability distribution of all the class in this position. If we think about it in sequence labeling, when we are labeling $o_t$, we already found the probability distribution of all classes for all previous observations from $o_1o_2…o_{t-1}$. Then we can classify $o_t$ with previous labels with HMM and more excitingly with other features!</p>

<p>So far, what we are talking about MaxEnt, we are talking about classifying based on given features, this sounds like a discriminative model right? And yes! <b>MaxEnt classifier is a discriminative classifier</b>. Therefore, we will change the iteration equation of Viterbi from HMM form to MaxEnt form, like from:</p>

<p>$Viterbi[t][i] = \argmax\limits_{1 &lt;j &lt;N} Viterbi[t-1][j]\cdot A[j][i]\cdot B(i, o_t)$</p>

<p>to:</p>

<p>$Viterbi[t][i] = \argmax\limits_{1 &lt;j &lt;N} Viterbi[t-1][j]\cdot P(s_j|s_i,o_t), 1 &lt; j &lt; N, 1 &lt; t &lt; T $</p>

<p>$=\argmax\limits_{1 &lt;j &lt;N} Viterbi[t-1][j]\cdot \vec{f}\cdot\vec{w}$</p>

<p>And that’s it!</p>
<h4 id="why-memm-over-hmm">Why MEMM over HMM?</h4>
<p>The reason is quite obvious. Because for HMM, we only derive state for our next sequence given the result of previous sequences. However, this does not work when we have some features only lies behind our current observation. MEMM can introduce these features while training its parameters and that’s a huge advantage.</p>

  </div>

  
</article>

      </div>
    </main>

    <footer class="site-footer">

  <div class="wrapper">

    <h2 class="footer-heading">Zili&#39;s Blog</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li>
            
              Zili&#39;s Blog
            
            </li>
            
            <li><a href="mailto:zx77@cornell.edu">zx77@cornell.edu</a></li>
            
        </ul>
      </div>

      <div class="footer-col footer-col-2">
        <ul class="social-media-list">
          
          <li>
            <a href="https://github.com/zxiang77"><span class="icon icon--github"><svg viewBox="0 0 16 16" width="16px" height="16px"><path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/></svg>
</span><span class="username">zxiang77</span></a>

          </li>
          

          
        </ul>
      </div>

      <div class="footer-col footer-col-3">
        <p>This is a place where Zili record things while learning</p>
      </div>
    </div>

  </div>

</footer>


  </body>

</html>
