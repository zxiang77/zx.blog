<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.5.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2017-09-17T12:58:09-07:00</updated><id>http://localhost:4000/</id><title type="html">Zili’s Blog</title><subtitle>This is a place where Zili record things while learning</subtitle><entry><title type="html">From HMM to MEMM</title><link href="http://localhost:4000/nlp/2017/09/16/from-HMM-to-MEMM.html" rel="alternate" type="text/html" title="From HMM to MEMM" /><published>2017-09-16T17:57:35-07:00</published><updated>2017-09-16T17:57:35-07:00</updated><id>http://localhost:4000/nlp/2017/09/16/from-HMM-to-MEMM</id><content type="html" xml:base="http://localhost:4000/nlp/2017/09/16/from-HMM-to-MEMM.html">&lt;p&gt;$\DeclareMathOperator*{\argmax}{arg\,max}$
Both HMM (Hidden Markov Model) and MEMM(MaxEnt Markov Model) are widely used in the field of NLP. This article is trying to articulate these two models and compare their differences. Most of my knowledge about them comes from &lt;a href=&quot;http://www.mit.edu/~6.863/spring2011/jmnew/6.pdf&quot;&gt;slides of MIT’s NLP course&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;hmm&quot;&gt;HMM&lt;/h3&gt;
&lt;p&gt;First thing first, HMM has two inter-dependent sequences: &lt;b&gt;observation&lt;/b&gt;, and &lt;b&gt;(hidden) states&lt;/b&gt; as we can indicate from the name of HMM (say with me: H-I-D-D-E-N Markov Model), the state changing lies behind what we can observe. e.g. POS (part of speech) tagging (states, as what lies behind) for a sentence (observation, what we see):&lt;/p&gt;

&lt;p&gt;observations: I like apple&lt;/p&gt;

&lt;p&gt;states: PRP VBP NN&lt;/p&gt;

&lt;p&gt;Given &lt;b&gt;observation&lt;/b&gt;, and &lt;b&gt; states&lt;/b&gt; defined, let’s define more important variables for HMM:&lt;/p&gt;

&lt;p&gt;$A$: &lt;b&gt;state changing matrix&lt;/b&gt;, with $A_{i,j}$ stands for the probability from state $i$ to state $j$&lt;/p&gt;

&lt;p&gt;$B$: &lt;b&gt;emission probability&lt;/b&gt;, with $B_{j,o_t}$ stands for the probability emission from state $j$ to observation $o_t$&lt;/p&gt;

&lt;h4 id=&quot;what-do-we-need-from-hmm&quot;&gt;What do we need from HMM?&lt;/h4&gt;
&lt;p&gt;There are three main things I will cover in this article:&lt;/p&gt;

&lt;p&gt;1) estimating probability of a sequence of states&lt;/p&gt;

&lt;p&gt;2) decoding, given observations and a HMM model $\lambda(A,B)$&lt;/p&gt;

&lt;p&gt;3) learning, given a set of states and observations, learn parameter $A$ and $B$ for HMM.&lt;/p&gt;

&lt;p&gt;I’ll explain why I need these now, but before that it is useful to know the difference between &lt;b&gt;generative model&lt;/b&gt; and &lt;b&gt;discriminative model&lt;/b&gt;. You can check this &lt;a href=&quot;http://localhost:4000ml/2017/09/15/discriminative-vs-generative-model.html&quot;&gt;post&lt;/a&gt; out.&lt;/p&gt;

&lt;h4 id=&quot;how-to-make-use-of-a-and-b&quot;&gt;How to Make Use of $A$ and $B$?&lt;/h4&gt;
&lt;p&gt;First of all, HMM is a generative model. That means it will do:&lt;/p&gt;

&lt;p&gt;$\argmax\limits_{S}P(O|S)P(S)$&lt;/p&gt;

&lt;p&gt;$=\argmax\limits_{S} \prod\limits_{s’,s\in S, o\in O} P(o|s)P(s)$&lt;/p&gt;

&lt;p&gt;$=\argmax\limits_{S} \prod\limits_{s’,s\in S, o\in O} B_{s,o}P(s’)A_{s’,s}$&lt;/p&gt;

&lt;p&gt;$P(s’)$ here stands for probability of previous observation’s ending in state $s’$.&lt;/p&gt;

&lt;p&gt;Actually, this term solves the first two of our problems: 1) getting probability and 2) getting most likely sequence.&lt;/p&gt;

&lt;h4 id=&quot;forward-algorithm-and-viterbi-algorithm&quot;&gt;Forward Algorithm and Viterbi Algorithm&lt;/h4&gt;
&lt;p&gt;So we got our objective in deriving probability and decoding, so how could we implement it? If we want to do it with brute force, it will be an $O(n^t)$ solution with $n$ stands for number of states, and $t$ stands for the length of the observations, which is not acceptable. However, it can be solved with dynamic programming with just $O(n\cdot t)$ time complexity, because we can derive the probability for each state in current observation with just all the probability of each state from the observation right before current observation. I am gonna just put the pseudo-code for forward and Viterbi algorithm from MIT slides here.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/Forward.png&quot; alt=&quot;Forward&quot; title=&quot;forward algorithm&quot; /&gt;
&lt;img src=&quot;/assets/Forward_trellis.png&quot; alt=&quot;Forward trellis&quot; title=&quot;Forward trellis&quot; /&gt;
&lt;img src=&quot;/assets/Viterbi.png&quot; alt=&quot;Viterbi algorithm&quot; title=&quot;Viterbi algorithm&quot; /&gt;
&lt;img src=&quot;/assets/Viterbi_trellis.png&quot; alt=&quot;Viterbi trellis&quot; title=&quot;Viterbi trellis&quot; /&gt;
/&lt;strong&gt;**&lt;/strong&gt;&lt;strong&gt;**&lt;/strong&gt;&lt;strong&gt;**&lt;/strong&gt;pseudo-code here &lt;strong&gt;**&lt;/strong&gt;&lt;strong&gt;**&lt;/strong&gt;*****/
Here we have to note that state 1 and  $N$ here are starting state and ending state, so we only deal with them during initialization and termination step, not in iteration step. Pseudo-code might consider them during iteration step, but it’s not actually considering them (there were a little bit inconsistency in the MIT slides when I was reading it, but just be aware of start and end state here)
As we can see, though both use dynamic programming, there are a few differences here:&lt;/p&gt;

&lt;p&gt;1) The iteration equation for forward algorithm and Viterbi are respectively:&lt;/p&gt;

&lt;p&gt;$forward[t][i] = \sum\limits_{1 &amp;lt;j &amp;lt;N} forward[t-1][j]\cdot A[j][i]$&lt;/p&gt;

&lt;p&gt;$Viterbi[t][i] = \argmax\limits_{1 &amp;lt;j &amp;lt;N} Viterbi[t-1][j]\cdot A[j][i]\cdot B(i, o_t)$&lt;/p&gt;

&lt;p&gt;$backpointer[t][i] = \argmax\limits_{1 &amp;lt;j &amp;lt; N} Viterbi[t-1][j]\cdot A[j][i]$ (again not considering $B(i, o_t)$ here because it does not affect the result)&lt;/p&gt;

&lt;p&gt;It is just because we want to get the probability of observations given the model $P(O|\lambda)$, this probability just sums up probabilities of all the possible sequences of states. While with Viterbi, we are only concerned with the one sequence of states that maximize the probability of the sequence.&lt;/p&gt;

&lt;h4 id=&quot;how-to-train-an-hmm&quot;&gt;How to train an HMM?&lt;/h4&gt;
&lt;p&gt;Here comes the most important part about the model. The quick answer is Expectation Maximization (EM) with forward-backward algorithm.
The long answer goes from here: for a normal Markov model with no hidden state, the state transformation can be easily computed by:
$\hat{A_{i,j}}=\frac{\text{count of all transformation from i to j}}{\text{count of all transformation from i}}$&lt;/p&gt;

&lt;p&gt;But we cannot do it with HMM because of hidden states. Therefore, we will need to derive hyper-parameters with some estimation with probability:&lt;/p&gt;

&lt;p&gt;$\hat{A_{i,j}}=\frac{\text{possibility of all sequences transform from i to j}}{\text{possibility of all sequences from i}}$&lt;/p&gt;

&lt;p&gt;How do we get the probability of all sequence from $i$ to $j$? The answer is - forward-backward algorithm!&lt;/p&gt;

&lt;p&gt;So as we already know that from forward algorithm, we know $\alpha[t][j]$ stands for the probability so far at $o_t$ of all the state sequences ending in state $j$. From this probability, we only know the probability of $o_1o_2…o_i…o_t$, but nothing behind $o_{t+1}…o_T$. That’s why we need a backward algorithm here to ‘foresee’ the probability of observations behind current observation.&lt;/p&gt;

&lt;p&gt;The backward algorithm is very similar to forward algorithm, except for one going from backward one from forward, so we just need to modify forward algorithm a little bit:&lt;/p&gt;

&lt;p&gt;1) Iterate from $T$ to $1$;&lt;/p&gt;

&lt;p&gt;2) Initialization: $\beta[T][i] = A_{iN}, 1&amp;lt;i&amp;lt;N$&lt;/p&gt;

&lt;p&gt;3) Recursion: $\beta[t][i] = \sum\limits_{1&amp;lt;j&amp;lt;N}A[i][j]\cdot B[j][o_{t+1}]\cdot\beta[t+1][j]$&lt;/p&gt;

&lt;p&gt;4) Termination: $\alpha[T][N] = \beta[1][1]= P(O|\lambda)=\sum\limits_{j=1}^{N-1}A[1][j]\cdot B[j][o_1]\cdot\beta(2, j)$&lt;/p&gt;

&lt;p&gt;Now we have everything we need for estimating sequence probability:&lt;/p&gt;

&lt;p&gt;define $\xi_t(i, j)$ as probability of being in state $i$ in $t$ and $j$ in $t+1$.&lt;/p&gt;

&lt;p&gt;$\xi_t(i, j)=\frac{\alpha[t][]\cdot A[i][j]\cdot \beta[t+1][j]\cdot B[j][o_{t+1}]}{\alpha[T][N]}$&lt;/p&gt;

&lt;p&gt;Similar for estimating $B$:&lt;/p&gt;

&lt;p&gt;$\hat{B}[j][v_k] = \frac{\text{expected count of observing $v_k$ on j}}{\text{expected count on state j}}$&lt;/p&gt;

&lt;p&gt;To estimate it we want to know the probability of being in state $j$ at time $t$, denoted as $\gamma[t][j]$:&lt;/p&gt;

&lt;p&gt;$\gamma[t][j] = \frac{P(q_t=j, O|\lambda)}{P(O|\lambda)}=\frac{\alpha[t][j]\cdot \beta[t][j]}{P(O|\lambda)}$&lt;/p&gt;

&lt;p&gt;and we can estimate $A$ with $xi$ and $B$ from it with $\gamma$. And pseudo-code below.
&lt;img src=&quot;/assets/forward_backward.png&quot; alt=&quot;forward-backward&quot; title=&quot;forward-backward algorithm&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Actually I was first confused by how to train the model when given a dataset of observations, because the code here only takes a singe sequence of observations into account. And I don’t know when we a dealing with many sequences of observations, could it still converge? My answer from my friend is “YES”, though he doesn’t know the proof either. So if anyone knows the proof, feel free to share it with me!&lt;/p&gt;

&lt;p&gt;###MEMM
Ok. So we have talked much about HMM and let’s briefly talk about MEMM (Max Entropy Markov Model). Before talking about MEMM, we should know MaxEnt classifier first. It is also called multinomial logistic regression. I will explain why later, but now I will focused more on how it works. It also works like logistic, because it takes a few observed features and output the distribution of probabilities among classes. &lt;b&gt;Entropy&lt;/b&gt; describes the average amount of information from a distribution. The higher it is, the less information we have. The entropy of a distribution $H(x)$ is computed as:&lt;/p&gt;

&lt;p&gt;$H(x)=-\sum\limits_{x} P(x)\cdot \log_2 P(x)$&lt;/p&gt;

&lt;p&gt;This sounds too abstract, check out this  &lt;a href=&quot;https://en.wikipedia.org/wiki/Entropy_(information_theory)#Example&quot;&gt;example&lt;/a&gt; on Wikipedia.&lt;/p&gt;

&lt;p&gt;As we can observe from the name, this classifier maximize the entropy of the training data, to achieve this, we have to deal with two cases:&lt;/p&gt;

&lt;p&gt;1) When we observed some features that makes the probability distribution among classes unevenly, such as it is more likely be class $c$ given feature $f$, then assign probability accordingly to class $c$;&lt;/p&gt;

&lt;p&gt;2) When we have no more features left, we just assign the rest of probability evenly to the rest of the classes.&lt;/p&gt;

&lt;p&gt;Actually, it turns out when we are maximizing the entropy of the model given the dataset, we are maximizing the probability of training data as well, that’s why MaxEnt is also called multinomial logistic regression.&lt;/p&gt;

&lt;p&gt;There is a thing we should note, is that the feature $f$ here, we should see it as a binary function, because it only indicates if the feature exists in the observation or not.&lt;/p&gt;

&lt;h4 id=&quot;classification&quot;&gt;Classification&lt;/h4&gt;
&lt;p&gt;So far the MaxEnt classifier only classify a single observation based on its features, so how do we implement it in  sequence labeling? The answer is MEMM. Actually we can implement Viterbi algorithm from HMM here with a little modification because they just share something in common. Assuming we are classifying observation $o_t$, the outcome is the probability distribution of all the class in this position. If we think about it in sequence labeling, when we are labeling $o_t$, we already found the probability distribution of all classes for all previous observations from $o_1o_2…o_{t-1}$. Then we can classify $o_t$ with previous labels with HMM and more excitingly with other features!&lt;/p&gt;

&lt;p&gt;So far, what we are talking about MaxEnt, we are talking about classifying based on given features, this sounds like a discriminative model right? And yes! &lt;b&gt;MaxEnt classifier is a discriminative classifier&lt;/b&gt;. Therefore, we will change the iteration equation of Viterbi from HMM form to MaxEnt form, like from:&lt;/p&gt;

&lt;p&gt;$Viterbi[t][i] = \argmax\limits_{1 &amp;lt;j &amp;lt;N} Viterbi[t-1][j]\cdot A[j][i]\cdot B(i, o_t)$&lt;/p&gt;

&lt;p&gt;to:&lt;/p&gt;

&lt;p&gt;$Viterbi[t][i] = \argmax\limits_{1 &amp;lt;j &amp;lt;N} Viterbi[t-1][j]\cdot P(s_j|s_i,o_t), 1 &amp;lt; j &amp;lt; N, 1 &amp;lt; t &amp;lt; T $&lt;/p&gt;

&lt;p&gt;$=\argmax\limits_{1 &amp;lt;j &amp;lt;N} Viterbi[t-1][j]\cdot \vec{f}\cdot\vec{w}$&lt;/p&gt;

&lt;p&gt;And that’s it!&lt;/p&gt;
&lt;h4 id=&quot;why-memm-over-hmm&quot;&gt;Why MEMM over HMM?&lt;/h4&gt;
&lt;p&gt;The reason is quite obvious. Because for HMM, we only derive state for our next sequence given the result of previous sequences. However, this does not work when we have some features only lies behind our current observation. MEMM can introduce these features while training its parameters and that’s a huge advantage.&lt;/p&gt;</content><author><name></name></author><summary type="html">$\DeclareMathOperator*{\argmax}{arg\,max}$ Both HMM (Hidden Markov Model) and MEMM(MaxEnt Markov Model) are widely used in the field of NLP. This article is trying to articulate these two models and compare their differences. Most of my knowledge about them comes from slides of MIT’s NLP course.</summary></entry><entry><title type="html">Generative Model vs. Deterministic Model</title><link href="http://localhost:4000/ml/2017/09/15/discriminative-vs-generative-model.html" rel="alternate" type="text/html" title="Generative Model vs. Deterministic Model" /><published>2017-09-15T17:57:35-07:00</published><updated>2017-09-15T17:57:35-07:00</updated><id>http://localhost:4000/ml/2017/09/15/discriminative-vs-generative-model</id><content type="html" xml:base="http://localhost:4000/ml/2017/09/15/discriminative-vs-generative-model.html">&lt;h3 id=&quot;generative-model-vs-deterministic-model&quot;&gt;Generative Model vs. Deterministic Model&lt;/h3&gt;
&lt;p&gt;In the world of machine learning, we actually deal with probability for the most of the time. To name a few, Naive Bayes, logistic regression. However, there are two main ways to deal with it, given observation $O$, and the sequence of states $S$ and the two ways are related to two different types of models in machine learning:&lt;/p&gt;

&lt;p&gt;1)&lt;b&gt;discriminative models&lt;/b&gt; maximize the chain probability given the observation (also called MLE - maximum likelihood estimation):&lt;/p&gt;

&lt;p&gt;$\DeclareMathOperator*{\argmax}{arg\,max}$
$\argmax\limits_{S}P(S|O)$&lt;/p&gt;

&lt;p&gt;2)&lt;b&gt;generative models&lt;/b&gt; optimize posterior probability given prior probability (also called MAP - maximize a posterior):&lt;/p&gt;

&lt;p&gt;$\argmax\limits_{S}P(S|O) $&lt;/p&gt;

&lt;p&gt;$=\argmax\limits_{S}\frac{P(O|S)P(S)}{P(O)}  $ (Bayes Rules)&lt;/p&gt;

&lt;p&gt;$=\argmax\limits_{S} P(O|S)P(S)$ remove $P(O)$ as that does not affect when $S$ can maximize the term&lt;/p&gt;

&lt;p&gt;This is the final posterior term we want to maximize:&lt;/p&gt;

&lt;p&gt;$\argmax\limits_{S} P(O|S)P(S)$&lt;/p&gt;

&lt;p&gt;Here, the $P(S)$ is the prior.&lt;/p&gt;

&lt;p&gt;P.S.: the discriminative and generative model’s connection with MLE v.s. MAP concepts are just my own words, I didn’t find anyone relate them together, though they share the same formula. If anyone knows any difference, please let me know!&lt;/p&gt;

&lt;p&gt;It is usually said that generative models are more flexible because it can be re-constructed to the form of $P(S|O)$, but the fact is, discriminative usually gives better performance than generative model. It is due to the principles behind these two types of models: 1) generative models model how the observations were generated and calculate which class gives the highest probability given the generation assumption ($P(x|y)P(y)$); 2) discriminative, however, only cares about classifying the data based on data ($P(y|x)$). These concepts makes generative models have more assumptions than discriminative models, so when its assumption does not stand well, its performance will be worse than discriminative models.&lt;/p&gt;</content><author><name></name></author><summary type="html">Generative Model vs. Deterministic Model In the world of machine learning, we actually deal with probability for the most of the time. To name a few, Naive Bayes, logistic regression. However, there are two main ways to deal with it, given observation $O$, and the sequence of states $S$ and the two ways are related to two different types of models in machine learning:</summary></entry><entry><title type="html">Perplexity is Actually Naive Bayes</title><link href="http://localhost:4000/nlp/2017/09/11/perplexity-is-actually-naive-bayes.html" rel="alternate" type="text/html" title="Perplexity is Actually Naive Bayes" /><published>2017-09-11T11:34:47-07:00</published><updated>2017-09-11T11:34:47-07:00</updated><id>http://localhost:4000/nlp/2017/09/11/perplexity-is-actually-naive-bayes</id><content type="html" xml:base="http://localhost:4000/nlp/2017/09/11/perplexity-is-actually-naive-bayes.html">&lt;h3 id=&quot;perplexity-is-actually-naive-bayes&quot;&gt;Perplexity is Actually Naive Bayes&lt;/h3&gt;
&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\DeclareMathOperator*{\argmin}{arg\,min}&lt;/script&gt;
&lt;script type=&quot;math/tex&quot;&gt;\DeclareMathOperator*{\argmax}{arg\,max}&lt;/script&gt;
&lt;script type=&quot;math/tex&quot;&gt;\DeclareMathOperator*{\sign}{sign}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Before everything, it’s for everyone’s good to define some of the notations at the very beginning:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Sentence of length $N$&lt;/strong&gt;: $W=w_1w_2…w_i…w_N$&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Perplexity&lt;/strong&gt;: $PP(W)=p(W)^{-1/N}=p(w_1w_2…w_N)^{-1/N}$&lt;/p&gt;

&lt;p&gt;Perplexity is usually used to test how much the test set correlates to a language model. The lower the perplexity, the more correlation the test set has with the language model. So, it is useful in classifying a sentence (negative/ positive reviews, topic classification, etc.) by training language models for different classification and compare their perplexity with test sentence. The test sentence is most likely to be in the class of the language model having the lowest perplexity with it.&lt;/p&gt;

&lt;p&gt;Then let’s talk about why its doing the same thing as Naive Bayes. I will demonstrate that by discussing a binary classification problem: positive/ negative reviews classification. It will be self-proving to extend it to multinomial classification after this.&lt;/p&gt;

&lt;p&gt;The language model way of doing it is to get:&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;$\argmin\limits_{c\in{-1,1}} PP(W&lt;/td&gt;
      &lt;td&gt;y=c) $&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;$= \argmin\limits_{c\in{-1,1}} p(W&lt;/td&gt;
      &lt;td&gt;y=c)^{-1/N}$&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;$=\argmax\limits_{c\in{-1,1}} p(W&lt;/td&gt;
      &lt;td&gt;y=c)$&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;While in Naive Bayes, we are exactly dealing with this!!&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;$\argmax\limits_{c\in{-1,1}} p(W&lt;/td&gt;
      &lt;td&gt;y=c)$&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Similarly, multinomial classification only replace $c\in{-1,1}$ with $c\in C$, with $C$ representing all the possible classes. Then it’s getting much more clear now.&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;$\argmax\limits_{c\in{-1,1}} p(W&lt;/td&gt;
      &lt;td&gt;y=c)$&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;$=\argmax\limits_{c\in{-1,1}} p(w_1|y=c)p(w_1|w_2,y=c)…p(w_i|w_{i+1},y=c)…p(w_{N-1}|w_N,y=c)$
(called introducing Naive Bayes rules for NB, and Ngram for language model)&lt;/p&gt;

&lt;p&gt;$=\argmax\limits_{c\in{-1,1}} \log p(w_0w_1|y=c)+\log p(w_1|w_2,y=c)+…+\log p(w_i|w_{i+1},y=c)+…+\log p(w_{N-1}|w_N,y=c)$
How do we relate this to n-gram model? If we see the above equation as $y = \vec{w}\vec{x}$, then let’s just think both $\vec{w}$ and $\vec{x}$ as vectors of size $V^n$ with $V$ representing vocabulary size. Then these vectors can represent any n-gram in the language model and it is straightforward that each value in the $\vec{w}$ represents the log-likelihood of certain n-gram, while each value in the $\vec{x}$ represents whether that n-gram appears in the sentence, so $\vec{x}[i]\in {1, -1}, (1\leq i\leq N)$. And from now on we can represent the correlation of a sentence with a language model by computing $y=\vec{w}\cdot\vec{x}$, the larger the $y$, the higher the probability, the more correlation they have.&lt;/p&gt;

&lt;p&gt;And for binary classification, we can have:&lt;/p&gt;

&lt;p&gt;$y_P=\vec{w_P}\cdot\vec{x}, y_N=\vec{w_N}\cdot\vec{x}$&lt;/p&gt;

&lt;p&gt;$y=\sign y_P\cdot P(y=1)-y_N\cdot P(y=-1) $&lt;/p&gt;

&lt;p&gt;$y=\sign (\vec{w_P}\cdot P(y=1)-\vec{w_N}\cdot P(y=-1))\cdot\vec{x}$, don’t forget the prior here&lt;/p&gt;

&lt;p&gt;$y=\sign \vec{w}\cdot\vec{x}, (\vec{w}=\vec{w_P}\cdot P(y=1)-\vec{w_N}\cdot P(y=-1))$&lt;/p&gt;

&lt;p&gt;Finally, we transformed the perplexity problem into a linear classifier form.&lt;/p&gt;</content><author><name></name></author><summary type="html">Perplexity is Actually Naive Bayes</summary></entry></feed>